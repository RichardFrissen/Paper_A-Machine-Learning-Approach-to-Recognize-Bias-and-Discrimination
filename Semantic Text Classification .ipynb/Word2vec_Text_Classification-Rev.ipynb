{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "municipal-given",
   "metadata": {},
   "source": [
    "## 1. Import libraries and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "played-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import nltk\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from statistics import mean\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed\n",
    "seed = np.random.seed(1)\n",
    "\n",
    "\n",
    "# Select Spacy model\n",
    "# Efficiency\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Accuracy\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assumed-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns and read annotated data\n",
    "columns = ['Token', 'Label', 'pos', 'ent_type', 'is_alpha', 'is_ascii', 'is_digit', 'is_lower', 'is_upper', 'is_title', 'is_punct', 'is_space', 'like_num', 'is_oov', 'is_stop', 'like_num', 'lang', 'sentiment']\n",
    "data = pd.read_csv('FULL_Annotation_data_output.tsv', sep='\\t', nrows=50, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "competent-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 196.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 256 ms, sys: 1.66 ms, total: 257 ms\n",
      "Wall time: 258 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Generate linguistic features for each token\n",
    "def feature_extraction(input_column):\n",
    "    features = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    for i in tqdm(input_column):\n",
    "        i = str(i)\n",
    "        doc = nlp(i)\n",
    "        word = i\n",
    "        for token in doc:\n",
    "            features[0].append(token.pos)\n",
    "            features[1].append(token.ent_type)\n",
    "            features[2].append(token.is_alpha)\n",
    "            features[3].append(token.is_ascii)\n",
    "            features[4].append(token.is_digit)\n",
    "            features[5].append(token.is_lower)\n",
    "            features[6].append(token.is_upper)\n",
    "            features[7].append(token.is_title)\n",
    "            features[8].append(token.is_punct)\n",
    "            features[9].append(token.is_space)\n",
    "            features[10].append(token.like_num)\n",
    "            features[11].append(token.is_oov)\n",
    "            features[12].append(token.is_stop)\n",
    "            features[13].append(token.lang)\n",
    "            features[14].append(token.sentiment)\n",
    "            features[15].append(len(word))\n",
    "    return features\n",
    "\n",
    "features = feature_extraction(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approximate-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.87 ms, sys: 1.73 ms, total: 7.59 ms\n",
      "Wall time: 7.51 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Label</th>\n",
       "      <th>pos</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_ascii</th>\n",
       "      <th>is_digit</th>\n",
       "      <th>is_lower</th>\n",
       "      <th>is_upper</th>\n",
       "      <th>is_title</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_space</th>\n",
       "      <th>like_num</th>\n",
       "      <th>is_oov</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>food</td>\n",
       "      <td>O</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fast</td>\n",
       "      <td>O</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grow</td>\n",
       "      <td>O</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-winne</td>\n",
       "      <td>O</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>online</td>\n",
       "      <td>O</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>food</td>\n",
       "      <td>O</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>community</td>\n",
       "      <td>O</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crowd</td>\n",
       "      <td>O</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14626626061804382878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Token Label  pos ent_type is_alpha is_ascii is_digit is_lower is_upper  \\\n",
       "0       food     O   92        0     True     True    False     True    False   \n",
       "1          a     O   95        0     True     True    False     True    False   \n",
       "2       fast     O   86        0     True     True    False     True    False   \n",
       "3       grow     O  100        0     True     True    False     True    False   \n",
       "4     -winne     O   97        0    False     True    False     True    False   \n",
       "5     online     O   86        0     True     True    False     True    False   \n",
       "6       food     O   92        0     True     True    False     True    False   \n",
       "7  community     O   92        0     True     True    False     True    False   \n",
       "8        and     O   89        0     True     True    False     True    False   \n",
       "9      crowd     O   92        0     True     True    False     True    False   \n",
       "\n",
       "  is_title is_punct is_space like_num is_oov is_stop                  lang  \\\n",
       "0    False    False    False    False   True   False  14626626061804382878   \n",
       "1    False    False    False    False   True    True  14626626061804382878   \n",
       "2    False    False    False    False   True   False  14626626061804382878   \n",
       "3    False    False    False    False   True   False  14626626061804382878   \n",
       "4    False    False    False    False   True   False  14626626061804382878   \n",
       "5    False    False    False    False   True   False  14626626061804382878   \n",
       "6    False    False    False    False   True   False  14626626061804382878   \n",
       "7    False    False    False    False   True   False  14626626061804382878   \n",
       "8    False    False    False    False   True    True  14626626061804382878   \n",
       "9    False    False    False    False   True   False  14626626061804382878   \n",
       "\n",
       "  sentiment word_length  \n",
       "0       0.0           4  \n",
       "1       0.0           1  \n",
       "2       0.0           4  \n",
       "3       0.0           4  \n",
       "4       0.0           6  \n",
       "5       0.0           6  \n",
       "6       0.0           4  \n",
       "7       0.0           9  \n",
       "8       0.0           3  \n",
       "9       0.0           5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert list to dataframe\n",
    "features = pd.DataFrame(features)\n",
    "\n",
    "# We need to transpose this dataframe first\n",
    "features = features.transpose()\n",
    "\n",
    "# We concat the annotated data with the linguistic features\n",
    "data = pd.concat([data, features], axis=1)\n",
    "data.columns = ['Token', 'Label', 'pos', 'ent_type', 'is_alpha', 'is_ascii', 'is_digit', 'is_lower', 'is_upper', 'is_title', 'is_punct', 'is_space', 'like_num', 'is_oov', 'is_stop', 'lang', 'sentiment', 'word_length']\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lesbian-upgrade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally the data can be saved to create a checkpoint\n",
    "\n",
    "data.to_csv('data_features_full_dataset.csv', index = False)\n",
    "\n",
    "data = pd.read_csv('data_features_full_dataset.csv', header = 0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bigger-scott",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3addb0377031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# init word embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \"\"\"\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# init word embedding\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate word embeddings for each token\n",
    "\n",
    "def word2vec_wordembed(input_column):\n",
    "    word2vec_result =[]\n",
    "    for i in tqdm(input_column):\n",
    "        try:\n",
    "            i = str(i)\n",
    "            result_array = word2vec[i]\n",
    "            result_list = result_array.tolist()\n",
    "            word2vec_result.append(result_list)\n",
    "        except KeyError:\n",
    "            word2vec_result.append(np.nan)\n",
    "        except TypeError: \n",
    "            word2vec_result.append(np.nan)\n",
    "\n",
    "\n",
    "    return word2vec_result\n",
    "\n",
    "\n",
    "# word2vec_word_embedding\n",
    "word_embedding = word2vec_wordembed(data['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f544f-6ec5-414a-a4f7-1b8501390982",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Replace empty values in the list of word embeddings for words \n",
    "# we couldn't generate a word embedding for\n",
    "\n",
    "word_embedding_complete = []\n",
    "\n",
    "for i in tqdm(word_embedding):\n",
    "    try:\n",
    "        if len(i) == 0:\n",
    "            i = [\"NaN\"]\n",
    "        else:\n",
    "            i = i\n",
    "        word_embedding_complete.append(i)\n",
    "        \n",
    "    except TypeError:\n",
    "        i = []\n",
    "        word_embedding_complete.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert list to dataframe\n",
    "word_embedding_df = pd.DataFrame(word_embedding_complete)\n",
    "word_embedding_series = word_embedding_df.apply(pd.Series)\n",
    "\n",
    "# We concat the annotated data with the linguistic features\n",
    "data = pd.concat([data, word_embedding_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Optionally the data can be saved to create a checkpoint\n",
    "\n",
    "data.to_csv('data_features_full_wordembedding_word2vec.csv', index = False)\n",
    "\n",
    "data = pd.read_csv('data_features_full_wordembedding_word2vec.csv', na_values=['nan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Replace NaN values with a \"0\"\n",
    "\n",
    "data = data.replace(np.nan, '0', regex=True)\n",
    "\n",
    "# We drop the token, as it is no longer needed for prediction\n",
    "data.drop('Token', axis=1, inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 80% / 20% split\n",
    "# Train, Test = train_test_split(data1, test_size=0.2, shuffle=False)\n",
    "\n",
    "X = data.drop(['Label'],axis=1).values # independant features\n",
    "y = data['Label'].values # dependant variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete data to save memory\n",
    "\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_iterations = 1000000000\n",
    "\n",
    "classifier = []\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "cv_classifier = []\n",
    "cv_precision = []\n",
    "cv_recall = []\n",
    "cv_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56f07b-eccd-4f1f-81ea-4b34dc547e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline\n",
    "\n",
    "clf = DummyClassifier(strategy=\"uniform\", random_state=seed)\n",
    "\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abb116-dcb3-42c5-8624-860beef5d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - Baseline\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('word2vec_baseline.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('word2vec_baseline.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d395f62-b970-4ea2-9a51-1cdd997e06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Baseline\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"Baseline\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "clf = LogisticRegression(solver='newton-cg', random_state=seed, max_iter=max_iterations)\n",
    "scores_LR = cross_validate(clf, X_train, y_train, scoring = scoring, cv=10, n_jobs=-1)\n",
    "LR_avg_precision = mean(scores_LR['test_precision_macro'])\n",
    "LR_avg_recall = mean(scores_LR['test_recall_macro'])\n",
    "LR_avg_f1 = mean(scores_LR['test_f1_macro'])\n",
    "\n",
    "cv_classifier.append(\"LR\")\n",
    "cv_precision.append(LR_avg_precision)\n",
    "cv_recall.append(LR_avg_recall)\n",
    "cv_f1.append(LR_avg_f1)\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb7b02-49fc-4d57-9767-e6777b421340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - LR\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('word2vec_lr.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('word2vec_lr.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Logistic Regression\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"LR\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "# Cross validation\n",
    "scoring = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "clf = DecisionTreeClassifier(random_state=seed)\n",
    "scores_DT = cross_validate(clf, X_train, y_train, scoring = scoring, cv=10, n_jobs=-1)\n",
    "DT_avg_precision = mean(scores_DT['test_precision_macro'])\n",
    "DT_avg_recall = mean(scores_DT['test_recall_macro'])\n",
    "DT_avg_f1 = mean(scores_DT['test_f1_macro'])\n",
    "\n",
    "cv_classifier.append(\"DT\")\n",
    "cv_precision.append(DT_avg_precision)\n",
    "cv_recall.append(DT_avg_recall)\n",
    "cv_f1.append(DT_avg_f1)\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78d655-af6f-49cb-91e6-c59912ac8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - DT\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('word2vec_dt.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('word2vec_dt.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Decision Tree\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"DT\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "clf = GaussianNB()\n",
    "scores_NB = cross_validate(clf, X_train, y_train, scoring = scoring, cv=10, n_jobs=-1)\n",
    "NB_avg_precision = mean(scores_NB['test_precision_macro'])\n",
    "NB_avg_recall = mean(scores_NB['test_recall_macro'])\n",
    "NB_avg_f1 = mean(scores_NB['test_f1_macro'])\n",
    "\n",
    "cv_classifier.append(\"NB\")\n",
    "cv_precision.append(NB_avg_precision)\n",
    "cv_recall.append(NB_avg_recall)\n",
    "cv_f1.append(NB_avg_f1)\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cdb269-0b3d-4612-963e-88c90e856ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - NB\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('word2vec_nb.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('word2vec_nb.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Naive Bayes\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"NB\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8b3ca-285e-4627-bd24-a8f56e908ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv = pd.DataFrame(zip(cv_classifier, cv_precision, cv_recall, cv_f1), columns = ['CV_Classifier', 'CV_Precision', 'CV_Recall', 'CV_F1-score'])\n",
    "results_cv = results_cv.sort_values(by = \"CV_F1-score\", ascending = False)\n",
    "\n",
    "f = open('word2vec_cv_results.pckl', 'wb')\n",
    "pickle.dump(results_cv, f)\n",
    "f.close()\n",
    "\n",
    "f = open('word2vec_cv_results.pckl', 'rb')\n",
    "results_cv = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742cc932-31ed-4bb2-8080-aa8b19b2c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(zip(classifier, accuracy, precision, recall, f1), columns = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
    "results = results.sort_values(by = \"F1-score\", ascending = False)\n",
    "\n",
    "f = open('word2vec_results.pckl', 'wb')\n",
    "pickle.dump(results, f)\n",
    "f.close()\n",
    "\n",
    "f = open('word2vec_results.pckl', 'rb')\n",
    "results = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9536d-b08f-484b-890a-ab5571ece660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results dataframe\n",
    "\n",
    "results.to_csv('word2vec_results.csv', index = False)\n",
    "results_cv.to_csv('word2vec_cv_results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
