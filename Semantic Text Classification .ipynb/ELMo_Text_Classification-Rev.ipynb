{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thousand-prairie",
   "metadata": {
    "id": "charming-junction"
   },
   "source": [
    "## 1. Import libraries and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d46bc-b7f7-4e41-bef8-e60aba9762e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp==0.9.0\n",
      "  Using cached allennlp-0.9.0-py3-none-any.whl (7.6 MB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (1.20.2)\n",
      "Collecting pytorch-transformers==1.1.0\n",
      "  Using cached pytorch_transformers-1.1.0-py3-none-any.whl (158 kB)\n",
      "Collecting flask>=1.0.2\n",
      "  Using cached Flask-2.0.2-py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (1.10.2)\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (0.4.2)\n",
      "Collecting spacy<2.2,>=2.1.0\n",
      "  Using cached spacy-2.1.9-cp38-cp38-linux_x86_64.whl\n",
      "Collecting jsonpickle\n",
      "  Using cached jsonpickle-2.1.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting jsonnet>=0.10.0\n",
      "  Using cached jsonnet-0.18.0-cp38-cp38-linux_x86_64.whl\n",
      "Collecting editdistance\n",
      "  Using cached editdistance-0.6.0-cp38-cp38-manylinux2010_x86_64.whl (286 kB)\n",
      "Collecting unidecode\n",
      "  Using cached Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "Requirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (4.60.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (3.2.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (0.24.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (1.6.3)\n",
      "Collecting responses>=0.7\n",
      "  Using cached responses-0.17.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Using cached tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (3.4.1)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.8/site-packages (from allennlp==0.9.0) (6.0.3)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting flask-cors>=3.0.7\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting pytorch-pretrained-bert>=0.6.0\n",
      "  Using cached pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n"
     ]
    }
   ],
   "source": [
    "# %pip install flair\n",
    "%pip install allennlp==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parliamentary-namibia",
   "metadata": {
    "id": "collected-warehouse"
   },
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "# import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "# import gensim\n",
    "# import gensim.downloader\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.test.utils import common_texts\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models.phrases import Phrases, Phraser\n",
    "# import nltk\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from statistics import mean\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import TransformerWordEmbeddings, ELMoEmbeddings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed\n",
    "seed = np.random.seed(1)\n",
    "\n",
    "\n",
    "# Select Spacy model\n",
    "# Efficiency\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Accuracy\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-benefit",
   "metadata": {
    "id": "smooth-fight"
   },
   "outputs": [],
   "source": [
    "# Define columns and read annotated data\n",
    "columns = ['Token', 'Label', 'pos', 'ent_type', 'is_alpha', 'is_ascii', 'is_digit', 'is_lower', 'is_upper', 'is_title', 'is_punct', 'is_space', 'like_num', 'is_oov', 'is_stop', 'like_num', 'lang', 'sentiment']\n",
    "data = pd.read_csv('FULL_Annotation_data_output.tsv', sep='\\t', nrows=50, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-trash",
   "metadata": {
    "id": "exceptional-gnome"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate linguistic features for each token\n",
    "def feature_extraction(input_column):\n",
    "    features = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    for i in input_column:\n",
    "        i = str(i)\n",
    "        doc = nlp(i)\n",
    "        word = i\n",
    "        for token in doc:\n",
    "            features[0].append(token.pos)\n",
    "            features[1].append(token.ent_type)\n",
    "            features[2].append(token.is_alpha)\n",
    "            features[3].append(token.is_ascii)\n",
    "            features[4].append(token.is_digit)\n",
    "            features[5].append(token.is_lower)\n",
    "            features[6].append(token.is_upper)\n",
    "            features[7].append(token.is_title)\n",
    "            features[8].append(token.is_punct)\n",
    "            features[9].append(token.is_space)\n",
    "            features[10].append(token.like_num)\n",
    "            features[11].append(token.is_oov)\n",
    "            features[12].append(token.is_stop)\n",
    "            features[13].append(token.lang)\n",
    "            features[14].append(token.sentiment)\n",
    "            features[15].append(len(word))\n",
    "    return features\n",
    "\n",
    "features = feature_extraction(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-stopping",
   "metadata": {
    "id": "still-contamination"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert list to dataframe\n",
    "features = pd.DataFrame(features)\n",
    "\n",
    "# We need to transpose this dataframe first\n",
    "features = features.transpose()\n",
    "\n",
    "# We concat the annotated data with the linguistic features\n",
    "data = pd.concat([data, features], axis=1)\n",
    "data.columns = ['Token', 'Label', 'pos', 'ent_type', 'is_alpha', 'is_ascii', 'is_digit', 'is_lower', 'is_upper', 'is_title', 'is_punct', 'is_space', 'like_num', 'is_oov', 'is_stop', 'lang', 'sentiment', 'word_length']\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fixed-botswana",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "generic-persian",
    "outputId": "38564dc5-71ef-4802-c5b9-7edefa85941b"
   },
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint\n",
    "\n",
    "# data.to_csv('data_features_full_dataset.csv', index = False)\n",
    "\n",
    "data = pd.read_csv('complete_data_features_full_dataset.csv', header = 0, skiprows=range(1, 500000), nrows = 250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tested-detection",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "passive-extreme",
    "outputId": "447612b5-4b90-4ca5-fffa-7516848f53b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init word embedding\n",
    "embedding = ELMoEmbeddings('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-uzbekistan",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biological-silly",
    "outputId": "49d4a6ba-d3e3-47e2-f2e2-de5b5f989adf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 173525/250000 [10:33:37<6:59:39,  3.04it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Generate word embeddings for each token\n",
    "\n",
    "def elmo_wordembed(input_column):\n",
    "    elmo_result =[]\n",
    "    counter = 0\n",
    "    for i in tqdm(input_column):\n",
    "        counter = counter +1\n",
    "        try:\n",
    "            i = str(i)\n",
    "            token = Sentence(i)\n",
    "            embedding.embed(token)\n",
    "            for token in token:\n",
    "                result_array = token.embedding\n",
    "            result_list = result_array.tolist()\n",
    "            elmo_result.append(result_list)\n",
    "        except KeyError:\n",
    "            elmo_result.append(np.nan)\n",
    "        except TypeError: \n",
    "            elmo_result.append(np.nan)\n",
    "        except IndexError:\n",
    "            elmo_result.append(np.nan)\n",
    "\n",
    "    return elmo_result\n",
    "\n",
    "\n",
    "# ELMo_Word_Embeddings\n",
    "word_embedding = elmo_wordembed(data['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a3591-ee49-4ef9-af90-67773ed8008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Replace empty values in the list of word embeddings for words \n",
    "# we couldn't generate a word embedding for\n",
    "\n",
    "word_embedding_complete = []\n",
    "\n",
    "for i in tqdm(word_embedding):\n",
    "    try:\n",
    "        if len(i) == 0:\n",
    "            i = []\n",
    "        else:\n",
    "            i = i\n",
    "        word_embedding_complete.append(i)\n",
    "        \n",
    "    except TypeError:\n",
    "        i = []\n",
    "        word_embedding_complete.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-ottawa",
   "metadata": {
    "id": "dramatic-canberra"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert list to dataframe\n",
    "word_embedding_df = pd.DataFrame(word_embedding_complete)\n",
    "word_embedding_series = word_embedding_df.apply(pd.Series)\n",
    "\n",
    "# We concat the annotated data with the linguistic features\n",
    "data = pd.concat([data, word_embedding_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-poster",
   "metadata": {
    "id": "applied-mineral"
   },
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-claim",
   "metadata": {
    "id": "shared-obligation"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Optionally the data can be saved to create a checkpoint\n",
    "\n",
    "data.to_csv('data_features_full_wordembedding_elmo_500_750.csv', index = False)\n",
    "\n",
    "# data = pd.read_csv('data_features_full_complete_wordembedding_elmo.csv', na_values=['nan'])\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df978cfd-82fa-4783-8171-ca3e2aa9321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2eedd1-07c0-466b-9cce-a09203afbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('complete_data_features_full_dataset.csv', header = 0, skiprows=range(1, 750000), nrows = 250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05cd19-bbc8-4d29-b58e-540836dd1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init word embedding\n",
    "embedding = ELMoEmbeddings('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844e10b-478f-4bc0-94a7-01a147a71ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate word embeddings for each token\n",
    "\n",
    "def elmo_wordembed(input_column):\n",
    "    elmo_result =[]\n",
    "    counter = 0\n",
    "    for i in tqdm(input_column):\n",
    "        counter = counter +1\n",
    "        try:\n",
    "            i = str(i)\n",
    "            token = Sentence(i)\n",
    "            embedding.embed(token)\n",
    "            for token in token:\n",
    "                result_array = token.embedding\n",
    "            result_list = result_array.tolist()\n",
    "            elmo_result.append(result_list)\n",
    "        except KeyError:\n",
    "            elmo_result.append(np.nan)\n",
    "        except TypeError: \n",
    "            elmo_result.append(np.nan)\n",
    "        except IndexError:\n",
    "            elmo_result.append(np.nan)\n",
    "\n",
    "    return elmo_result\n",
    "\n",
    "\n",
    "# ELMo_Word_Embeddings\n",
    "word_embedding = elmo_wordembed(data['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ea178-ed30-4024-8291-56dcbdd72b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Replace empty values in the list of word embeddings for words \n",
    "# we couldn't generate a word embedding for\n",
    "\n",
    "word_embedding_complete = []\n",
    "\n",
    "for i in tqdm(word_embedding):\n",
    "    try:\n",
    "        if len(i) == 0:\n",
    "            i = []\n",
    "        else:\n",
    "            i = i\n",
    "        word_embedding_complete.append(i)\n",
    "        \n",
    "    except TypeError:\n",
    "        i = []\n",
    "        word_embedding_complete.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723d497-7744-41e3-a689-5e588ed93bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert list to dataframe\n",
    "word_embedding_df = pd.DataFrame(word_embedding)\n",
    "word_embedding_series = word_embedding_df[0].apply(pd.Series)\n",
    "\n",
    "# We concat the annotated data with the linguistic features\n",
    "data = pd.concat([data, word_embedding_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be1955-ed21-4992-8fba-e4d4fd27467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_features_full_wordembedding_elmo_750_1000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa38f9a-0d2e-4790-b6e2-a52b649cf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data)\n",
    "del(embedding)\n",
    "del(word_embedding)\n",
    "del(word_embedding_df)\n",
    "del(word_embedding_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-timer",
   "metadata": {
    "id": "involved-senior"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Replace NaN values with a \"0\"\n",
    "\n",
    "data = data.replace(np.nan, '0', regex=True)\n",
    "\n",
    "# We drop the token, as it is no longer needed for prediction\n",
    "data.drop('Token', axis=1, inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-peripheral",
   "metadata": {
    "id": "cross-protocol"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 80% / 20% split\n",
    "# Train, Test = train_test_split(data1, test_size=0.2, shuffle=False)\n",
    "\n",
    "X = data.drop(['Label'],axis=1).values # independant features\n",
    "y = data['Label'].values # dependant variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-cheat",
   "metadata": {
    "id": "infectious-certificate"
   },
   "outputs": [],
   "source": [
    "# Delete data to save memory\n",
    "\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-coordinate",
   "metadata": {
    "id": "prostate-equilibrium"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_iterations = 1000000000\n",
    "\n",
    "classifier = []\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "cv_classifier = []\n",
    "cv_precision = []\n",
    "cv_recall = []\n",
    "cv_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f146c7-8eff-425e-99d0-d4472305c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline\n",
    "\n",
    "clf = DummyClassifier(strategy=\"uniform\", random_state=seed)\n",
    "\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b010d6-e333-419a-abc2-e893f9f5b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - Baseline\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('elmo_baseline.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('elmo_baseline.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4390a-b75b-43de-a9c0-e4f1aa5f4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Baseline\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"Baseline\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-blowing",
   "metadata": {
    "id": "consolidated-springer"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "clf = LogisticRegression(solver='newton-cg', random_state=seed, max_iter=max_iterations)\n",
    "scores_LR = cross_validate(clf, X_train, y_train, scoring = scoring, cv=10, n_jobs=-1)\n",
    "LR_avg_precision = mean(scores_LR['test_precision_macro'])\n",
    "LR_avg_recall = mean(scores_LR['test_recall_macro'])\n",
    "LR_avg_f1 = mean(scores_LR['test_f1_macro'])\n",
    "\n",
    "cv_classifier.append(\"LR\")\n",
    "cv_precision.append(LR_avg_precision)\n",
    "cv_recall.append(LR_avg_recall)\n",
    "cv_f1.append(LR_avg_f1)\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca7772-986b-436c-a1ad-5b1ab14d2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - LR\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('elmo_lr.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('elmo_lr.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3b97a-7848-48cf-8293-db85a2859996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Logistic Regression\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"LR\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-pillow",
   "metadata": {
    "id": "elegant-destiny"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "# Cross validation\n",
    "scoring = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "clf = DecisionTreeClassifier(random_state=seed)\n",
    "scores_DT = cross_validate(clf, X_train, y_train, scoring = scoring, cv=10, n_jobs=-1)\n",
    "DT_avg_precision = mean(scores_DT['test_precision_macro'])\n",
    "DT_avg_recall = mean(scores_DT['test_recall_macro'])\n",
    "DT_avg_f1 = mean(scores_DT['test_f1_macro'])\n",
    "\n",
    "cv_classifier.append(\"DT\")\n",
    "cv_precision.append(DT_avg_precision)\n",
    "cv_recall.append(DT_avg_recall)\n",
    "cv_f1.append(DT_avg_f1)\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644d758-8a48-47b5-828f-b6f09847e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - DT\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('elmo_dt.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('elmo_dt.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-packaging",
   "metadata": {
    "id": "incorporated-baseball"
   },
   "outputs": [],
   "source": [
    "# Evaluation Decision Tree\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"DT\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-ozone",
   "metadata": {
    "id": "failing-marking"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "clf = GaussianNB()\n",
    "scores_NB = cross_validate(clf, X_train, y_train, scoring = scoring, cv=10, n_jobs=-1)\n",
    "NB_avg_precision = mean(scores_NB['test_precision_macro'])\n",
    "NB_avg_recall = mean(scores_NB['test_recall_macro'])\n",
    "NB_avg_f1 = mean(scores_NB['test_f1_macro'])\n",
    "\n",
    "cv_classifier.append(\"NB\")\n",
    "cv_precision.append(NB_avg_precision)\n",
    "cv_recall.append(NB_avg_recall)\n",
    "cv_f1.append(NB_avg_f1)\n",
    "\n",
    "# Model fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9a2ac-83ba-458a-9c6c-4c31929ecec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally the data can be saved to create a checkpoint - NB\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('elmo_nb.pckl', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "\n",
    "f = open('elmo_nb.pckl', 'rb')\n",
    "clf = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-lighter",
   "metadata": {
    "id": "different-statement"
   },
   "outputs": [],
   "source": [
    "# Evaluation Naive Bayes\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1_score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "classifier.append(\"NB\")\n",
    "accuracy.append(accuracy_score(y_test, y_pred))\n",
    "precision.append(precision_score(y_test, y_pred, average='macro',zero_division=0))\n",
    "recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f5208-4d60-4680-8be3-c1801683bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv = pd.DataFrame(zip(cv_classifier, cv_precision, cv_recall, cv_f1), columns = ['CV_Classifier', 'CV_Precision', 'CV_Recall', 'CV_F1-score'])\n",
    "results_cv = results_cv.sort_values(by = \"CV_F1-score\", ascending = False)\n",
    "\n",
    "f = open('elmo_cv_results.pckl', 'wb')\n",
    "pickle.dump(results_cv, f)\n",
    "f.close()\n",
    "\n",
    "f = open('elmo_cv_results.pckl', 'rb')\n",
    "results_cv = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1159df-6f6b-4360-80c3-e6b47bc68029",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(zip(classifier, accuracy, precision, recall, f1), columns = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
    "results = results.sort_values(by = \"F1-score\", ascending = False)\n",
    "\n",
    "f = open('elmo_results.pckl', 'wb')\n",
    "pickle.dump(results, f)\n",
    "f.close()\n",
    "\n",
    "f = open('elmo_results.pckl', 'rb')\n",
    "results = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e9738a-38cc-4e35-9b7e-4ea7933a06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results dataframe\n",
    "\n",
    "results.to_csv('elmo_results.csv', index = False)\n",
    "results_cv.to_csv('elmo_cv_results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ELMo_Text_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
